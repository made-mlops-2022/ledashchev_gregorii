Airflow ml-model pipeline
---
1. Ежедневно генерируются данные. 
2. Мы их используем для обучения МЛ модельки для задачи классификации. <br>
3. Еженедельно, мы переобучаем модель на новых данных. <br>
4. Ежедневно, текущая выбранная нами модель скорит данные и записывает предсказания.

## Запуск сервера airflow
### Задать airflow-переменные:
Нужно задать актуальные значения папок на локальной машине, куда будут складываться данные и модель.<br>
Для этого нужно отредактировать соответствующие строки в `docker-compose.yaml`(строки 12-17):
```
# LIST YOUR VARIABLES HERE:
  # path to local folder for data storing
  - AIRFLOW_VAR_PATH_TO_DATA=/home/gregor/made/my_flow/data
  # path to local folder for model storing (in universe of healthy people the following two variables are equal):
  - AIRFLOW_VAR_PATH_TO_MODEL_SAVING=/data/model/{{ ds }}
  - AIRFLOW_VAR_PATH_TO_MODEL_LOADING=/data/model/{{ ds }}
```
`AIRFLOW_VAR_PATH_TO_DATA` - путь для сохранения данных. <br>
`AIRFLOW_VAR_PATH_TO_MODEL_SAVING` - путь для сохранения свежеобученной модели. <br>
`AIRFLOW_VAR_PATH_TO_MODEL_LOADING` - путь для загрузки модели, с помощью которой мы хотим предсказывать на проде. <br>

### Выполнить:
```commandline
docker-compose up --build
```

## Тестирование сервера:
1. Открыть в браузере http://localhost:8080/home <br>
2. Запустить DAG `generate_data` <br>
*После его выполнения в директории .../data появится директория ...data/raw/{CURRENT_DATE} в которой будут находится свежие data.csv и target.csv*
3. Запустить DAG `model_fitting` <br>
*Данные перемещаются в папку processed, к данным добавляется столбец dummy, заполненный единицами, данные делятся на трейн и тест, модель обучается и валидируется.* <br>
*После его выполнения в .../data появятся директории processed, split, validation, model. Сама модель сохранена в ...data/model/{CURRENT_DATE}/model.joblib*
4. Запустить DAG `predict` <br>
*После его выполнения появится директория /data/predict, в которой будет .... predict))))*

## Структура проекта
```

├── dags <<<<<<<<<<<<<<<<<<<<<<<<<< dags
│       ├── 01_dag_get_data.py
│       ├── 02_dag_model_fitting_pipeline.py
│       ├── 03_dag_prediction.py
├── data <<<<<<<<<<<<<<<<<<<<<<<<<< all data generated by dags
├── docker-compose.yml
├── images <<<<<<<<<<<<<<<<<<<<<<<<<< docker images
│       ├── airflow-docker
│       │   └── Dockerfile
│       ├── airflow-fit
│       │   ├── app
│       │   │   └── fit_model.py
│       │   ├── Dockerfile
│       │   └── requirements.txt
.       .
.       .
.       .
└── README.md

```